{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision as vision\n",
    "from torchvision import transforms, utils, datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [x[0] for x in os.walk('ILSVRC/Data/CLS-LOC/train')] # obtain all subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = categories[1:] # eliminate root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [path.split('/')[4] for path in categories] # obtain subfolder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryMap = {i : j for j, i in enumerate(categories, 0)} # create category mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate data class\n",
    "\n",
    "class ImageNetData(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, categoryMap, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv_file containing labels\n",
    "            root_dir (string): directory containing all data\n",
    "            transform (callable, optional): optional transforms to be applied to an image\n",
    "        \"\"\"\n",
    "        # process labels\n",
    "        self.trainLabels = pd.read_csv(csv_file) # read label file\n",
    "        self.trainLabels['PredictionString'] = self.trainLabels['PredictionString'].str.split(n=1).str.get(0) # process so only category labels remain\n",
    "        self.trainLabels['PredictionString'] = self.trainLabels['PredictionString'].map(categoryMap).astype(np.long) # map labels to category indices\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trainLabels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_folder = self.trainLabels['ImageId'].iloc[idx].split('_')[0]\n",
    "        img_name = self.trainLabels['ImageId'].iloc[idx] + '.JPEG'\n",
    "        img_path = os.path.join(self.root_dir, img_folder, img_name)\n",
    "        \n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert('RGB')\n",
    "        label = np.array(self.trainLabels['PredictionString'].iloc[idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        sample = {'image': image, 'label': label}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate validation data class\n",
    "class ImageNetValidation(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, categoryMap, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): csv_file containing labels\n",
    "            root_dir (string): directory containing all data\n",
    "            transform (callable, optional): optional transforms to be applied to an image\n",
    "        \"\"\"\n",
    "        # process labels\n",
    "        self.valLabels = pd.read_csv(csv_file) # read label file\n",
    "        self.valLabels['PredictionString'] = self.valLabels['PredictionString'].str.split(n=1).str.get(0) # process so only category labels remain\n",
    "        self.valLabels['PredictionString'] = self.valLabels['PredictionString'].map(categoryMap).astype(np.long) # map labels to category indices\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.valLabels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        #img_folder = self.valLabels['ImageId'].iloc[idx].split('_')[0]\n",
    "        img_name = self.valLabels['ImageId'].iloc[idx] + '.JPEG'\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        \n",
    "        image = Image.open(img_path)\n",
    "        image = image.convert('RGB')\n",
    "        label = np.array(self.valLabels['PredictionString'].iloc[idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        sample = {'image': image, 'label': label}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imageData = ImageNetData(csv_file = 'LOC_train_solution.csv', root_dir = 'ILSVRC/Data/CLS-LOC/train')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# show images\n",
    "for i in range(4):\n",
    "    sample = imageData[i]\n",
    "    \n",
    "    #print(i, sample['image'].shape, sample['label'].shape)\n",
    "    \n",
    "    ax = plt.subplot(4, 1, i+1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample {}'.format(i))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(sample['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valData = ImageNetValidation(csv_file = 'LOC_val_solution.csv', root_dir = 'ILSVRC/Data/CLS-LOC/val')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# show images\n",
    "for i in range(4):\n",
    "    sample = valData[i]\n",
    "    \n",
    "    #print(i, sample['image'].shape, sample['label'].shape)\n",
    "    \n",
    "    ax = plt.subplot(4, 1, i+1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample {}'.format(i))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(sample['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create image transformations\n",
    "train_transform = transforms.Compose([transforms.Resize(size=(336, 336)),\n",
    "                                      transforms.RandomRotation(degrees=20),\n",
    "                                      transforms.RandomCrop(size=224),\n",
    "                                      transforms.RandomVerticalFlip(p=0.5),\n",
    "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "                                      transforms.RandomErasing(scale=(0.02, 0.1))])\n",
    "\n",
    "val_transform = transforms.Compose([transforms.Resize(size=(224, 224)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see the effects of transformation\n",
    "#imageData = ImageNetData(csv_file = 'LOC_train_solution.csv', root_dir = 'ILSVRC/Data/CLS-LOC/train')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# show images\n",
    "for i in range(4):\n",
    "    sample = imageData[i]\n",
    "    \n",
    "    transformed_image = train_transform(sample['image'])\n",
    "    \n",
    "    print(i, transformed_image.shape, sample['label'].shape)\n",
    "    \n",
    "    ax = plt.subplot(4, 1, i+1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample {}'.format(i))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(transformed_image.numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fig = plt.figure()\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "# show images\n",
    "for i in range(4):\n",
    "    sample = valData[i]\n",
    "    \n",
    "    transformed_image = val_transform(sample['image'])\n",
    "    \n",
    "    print(i, transformed_image.shape, sample['label'].shape)\n",
    "    \n",
    "    ax = plt.subplot(4, 1, i+1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample {}'.format(i))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(transformed_image.numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainLabelFile = 'LOC_train_solution.csv'\n",
    "train_dir = 'ILSVRC/Data/CLS-LOC/train'\n",
    "valLabelFile = 'LOC_val_solution.csv'\n",
    "val_dir = 'ILSVRC/Data/CLS-LOC/val'\n",
    "ImageNetDataset = ImageNetData(csv_file = trainLabelFile, root_dir = train_dir, transform = train_transform)\n",
    "train_loader = DataLoader(ImageNetDataset, batch_size=512, shuffle=True, num_workers=16)\n",
    "ImageNetVal = ImageNetValidation(csv_file = valLabelFile, root_dir = val_dir, transform = val_transform)\n",
    "val_loader = DataLoader(ImageNetVal, batch_size=512, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(len(ImageNetDataset)):\n",
    "    img = ImageNetDataset[i]\n",
    "    if img['image'].shape[0] != 3:\n",
    "        print(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_batch = next(iter(train_loader))\n",
    "grid = utils.make_grid(image_batch['image'])\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_batch = next(iter(val_loader))\n",
    "grid = utils.make_grid(val_batch['image'])\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(grid.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data, lab = image_batch\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(batch['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "class deepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(deepNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 128, 3, stride=2)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3)\n",
    "        self.batchnorm4 = nn.BatchNorm2d(128)\n",
    "        self.conv5 = nn.Conv2d(128, 384, 3, stride=2)\n",
    "        self.inceptionA_batchnorm = nn.BatchNorm2d(384)\n",
    "        self.inceptionA_conv11 = nn.Conv2d(384, 32, 1)\n",
    "        self.inceptionA_batchnorm11 = nn.BatchNorm2d(32)\n",
    "        self.inceptionA_conv12 = nn.Conv2d(384, 32, 1)\n",
    "        self.inceptionA_batchnorm12 = nn.BatchNorm2d(32)\n",
    "        self.inceptionA_conv13 = nn.Conv2d(384, 32, 1)\n",
    "        self.inceptionA_batchnorm13 = nn.BatchNorm2d(32)\n",
    "        self.inceptionA_conv31 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.inceptionA_batchnorm31 = nn.BatchNorm2d(32)\n",
    "        self.inceptionA_conv32 = nn.Conv2d(32, 48, 3, padding=1)\n",
    "        self.inceptionA_batchnorm32 = nn.BatchNorm2d(48)\n",
    "        self.inceptionA_conv33 = nn.Conv2d(48, 64, 3, padding=1)\n",
    "        self.inceptionA_batchnorm33 = nn.BatchNorm2d(64)\n",
    "        self.inceptionA_conv14 = nn.Conv2d(128, 384, 1)\n",
    "        self.reductionA_conv6 = nn.Conv2d(384, 1154, 1)\n",
    "        self.inceptionB_batchnorm = nn.BatchNorm2d(1154)\n",
    "        self.inceptionB_conv11 = nn.Conv2d(1154, 192, 1)\n",
    "        self.inceptionB_batchnorm11 = nn.BatchNorm2d(192)\n",
    "        self.inceptionB_conv12 = nn.Conv2d(1154, 128, 1)\n",
    "        self.inceptionB_batchnorm12 = nn.BatchNorm2d(128)\n",
    "        self.inceptionB_conv71 = nn.Conv2d(128, 160, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.inceptionB_batchnorm71 = nn.BatchNorm2d(160)\n",
    "        self.inceptionB_conv72 = nn.Conv2d(160, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.inceptionB_batchnorm72 = nn.BatchNorm2d(192)\n",
    "        self.inceptionB_conv13 = nn.Conv2d(384, 1154, 1)\n",
    "        self.reductionB_conv7 = nn.Conv2d(1154, 2048, 1)\n",
    "        self.inceptionC_batchnorm = nn.BatchNorm2d(2048)\n",
    "        self.inceptionC_conv11 = nn.Conv2d(2048, 192, 1)\n",
    "        self.inceptionC_batchnorm11 = nn.BatchNorm2d(192)\n",
    "        self.inceptionC_conv12 = nn.Conv2d(2048, 192, 1)\n",
    "        self.inceptionC_batchnorm12 = nn.BatchNorm2d(192)\n",
    "        self.inceptionC_conv31 = nn.Conv2d(192, 224, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.inceptionC_batchnorm31 = nn.BatchNorm2d(224)\n",
    "        self.inceptionC_conv32 = nn.Conv2d(224, 256, kernel_size=(3, 1), padding=(1, 0))\n",
    "        self.inceptionC_batchnorm32 = nn.BatchNorm2d(256)\n",
    "        self.inceptionC_conv13 = nn.Conv2d(448, 2048, 1)\n",
    "        self.batchnorm5 = nn.BatchNorm1d(2048)\n",
    "        self.fc1000 = nn.Linear(2048, 1000)\n",
    "        \n",
    "    def inceptionA(self, x):\n",
    "        x = F.relu(self.inceptionA_batchnorm(x))\n",
    "        x1 = self.inceptionA_conv11(x)\n",
    "        x1 = F.relu(self.inceptionA_batchnorm11(x1))\n",
    "        x2 = self.inceptionA_conv12(x)\n",
    "        x2 = F.relu(self.inceptionA_batchnorm12(x2))\n",
    "        x2 = self.inceptionA_conv31(x2)\n",
    "        x2 = F.relu(self.inceptionA_batchnorm31(x2))\n",
    "        x3 = self.inceptionA_conv13(x)\n",
    "        x3 = F.relu(self.inceptionA_batchnorm13(x3))\n",
    "        x3 = self.inceptionA_conv32(x3)\n",
    "        x3 = F.relu(self.inceptionA_batchnorm32(x3))\n",
    "        x3 = self.inceptionA_conv33(x3)\n",
    "        x3 = F.relu(self.inceptionA_batchnorm33(x3))\n",
    "        x_concat = torch.cat((x1, x2, x3), 1)\n",
    "        out = self.inceptionA_conv14(x_concat)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def inceptionB(self, x):\n",
    "        x = F.relu(self.inceptionB_batchnorm(x))\n",
    "        x1 = self.inceptionB_conv11(x)\n",
    "        x1 = F.relu(self.inceptionB_batchnorm11(x1))\n",
    "        x2 = self.inceptionB_conv12(x)\n",
    "        x2 = F.relu(self.inceptionB_batchnorm12(x2))\n",
    "        x2 = self.inceptionB_conv71(x2)\n",
    "        x2 = F.relu(self.inceptionB_batchnorm71(x2))\n",
    "        x2 = self.inceptionB_conv72(x2)\n",
    "        x2 = F.relu(self.inceptionB_batchnorm72(x2))\n",
    "        x_concat = torch.cat((x1, x2), 1)\n",
    "        out = self.inceptionB_conv13(x_concat)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def inceptionC(self, x):\n",
    "        x = F.relu(self.inceptionC_batchnorm(x))\n",
    "        x1 = self.inceptionC_conv11(x)\n",
    "        x1 = F.relu(self.inceptionC_batchnorm11(x1))\n",
    "        x2 = self.inceptionC_conv12(x)\n",
    "        x2 = F.relu(self.inceptionC_batchnorm12(x2))\n",
    "        x2 = self.inceptionC_conv31(x2)\n",
    "        x2 = F.relu(self.inceptionC_batchnorm31(x2))\n",
    "        x2 = self.inceptionC_conv32(x2)\n",
    "        x2 = F.relu(self.inceptionC_batchnorm32(x2))\n",
    "        x_concat = torch.cat((x1, x2), 1)\n",
    "        out = self.inceptionC_conv13(x_concat)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.batchnorm1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.batchnorm2(x))\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.batchnorm3(x))\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.batchnorm4(x))\n",
    "        x = self.conv5(x)\n",
    "        x = F.max_pool2d(x, kernel_size=(2,2), stride=2)\n",
    "        x = F.relu(self.inceptionA_batchnorm(x))\n",
    "        x_res1 = self.inceptionA(x)\n",
    "        x_res1 = x + x_res1\n",
    "        x_res1 = self.reductionA_conv6(x_res1)\n",
    "        x_res1 = F.max_pool2d(x_res1, kernel_size=(2,2), stride=2)\n",
    "        x_res2 = self.inceptionB(x_res1)\n",
    "        x_res2 = x_res2 + x_res1\n",
    "        x_res2 = self.reductionB_conv7(x_res2)\n",
    "        x_res2 = F.max_pool2d(x_res2, kernel_size=(2,2), stride=2)\n",
    "        x_res3 = self.inceptionC(x_res2)\n",
    "        x_res3 = x_res3 + x_res2\n",
    "        x_res3 = F.max_pool2d(x_res3, kernel_size=(6,6))\n",
    "        x_res3 = torch.flatten(x_res3, start_dim=1)\n",
    "        x_res3 = F.relu(self.batchnorm5(x_res3))\n",
    "        out = self.fc1000(x_res3)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "start_epoch = 0\n",
    "best_val_acc = 0\n",
    "lr = 0.001\n",
    "save_dir = \"Models\"\n",
    "val_conv_net = \"Inception_Resnet_custom_bestval\"\n",
    "train_conv_net = \"Inception_Resnet_custom_train\"\n",
    "Start_From_Checkpoint = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabelFile = 'LOC_train_solution.csv'\n",
    "train_dir = 'ILSVRC/Data/CLS-LOC/train'\n",
    "valLabelFile = 'LOC_val_solution.csv'\n",
    "val_dir = 'ILSVRC/Data/CLS-LOC/val'\n",
    "ImageNetDataset = ImageNetData(csv_file = trainLabelFile, root_dir = train_dir, categoryMap = categoryMap, transform = train_transform)\n",
    "train_loader = DataLoader(ImageNetDataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "ImageNetVal = ImageNetValidation(csv_file = valLabelFile, root_dir = val_dir, categoryMap = categoryMap, transform = val_transform)\n",
    "val_loader = DataLoader(ImageNetVal, batch_size=batch_size, shuffle=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_train = len(ImageNetDataset)\n",
    "length_val = len(ImageNetVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveVal = os.path.join(save_dir, val_conv_net + '.pt')\n",
    "SaveTrain = os.path.join(save_dir, train_conv_net + '.pt')\n",
    "\n",
    "#Create the save directory if it does note exist\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use  2  GPUs!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = deepNet()\n",
    "if (torch.cuda.device_count() > 1):\n",
    "    print(\"Let's use \", torch.cuda.device_count(), \" GPUs!\\n\")\n",
    "    net = nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy calculator\n",
    "def calculate_accuracy(fx, y):\n",
    "    preds = fx.max(1, keepdim=True)[1]\n",
    "    correct = preds.eq(y.view_as(preds)).sum()\n",
    "    acc = correct.float()/preds.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss and optimiser, and scheduler (for lr adjustment)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE TRAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train function\n",
    "def train(net, device, criterion, optimizer, train_loader, epoch, length_train):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inp = data['image']\n",
    "        lab = data['label']\n",
    "        \n",
    "        inp = inp.to(device)\n",
    "        lab = lab.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward propagation\n",
    "        out = net.forward(inp)\n",
    "        loss = criterion(out, lab)\n",
    "        acc = calculate_accuracy(out, lab)\n",
    "        \n",
    "        # backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # update network\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        epoch_acc += acc\n",
    "        \n",
    "        #print(f\"TRAINING  |  ITER: {i+1:02}  |  LOSS: {loss.item():.3f}\", end = '\\r')\n",
    "        print(f'EPOCH {epoch+1:02}  |  ITER {i+1:02}/{length_train//128+1:02}  |  Training loss {loss.item():.3f}  |  Training acc {acc*100:.3f}', end='\\r')\n",
    "        \n",
    "    return epoch_loss/len(train_loader), epoch_acc/len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE EVALUATION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation function\n",
    "def evaluate(net, device, criterion, test_loader, length_test):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            inp = data['image']\n",
    "            lab = data['label']\n",
    "            inp = inp.to(device)\n",
    "            lab = lab.to(device)\n",
    "            \n",
    "            # forward propagation\n",
    "            out = net(inp)\n",
    "            loss = criterion(out, lab)\n",
    "            acc = calculate_accuracy(out, lab)\n",
    "            \n",
    "            epoch_loss += loss\n",
    "            epoch_acc += acc\n",
    "                \n",
    "            print(f\"EVALUATION  |  ITER: {i+1:02}/{length_test//128+1:02}  |  LOSS: {loss.item():.3f}\", end = '\\r')\n",
    "            \n",
    "    return epoch_loss/len(test_loader), epoch_acc/len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_log = []\n",
    "train_acc_log = []\n",
    "validation_loss_log = []\n",
    "validation_acc_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loaded, starting from epoch: 74\n"
     ]
    }
   ],
   "source": [
    "#Load Checkpoint\n",
    "if Start_From_Checkpoint:\n",
    "    #Check if checkpoint exists\n",
    "    if os.path.isfile(SaveTrain):\n",
    "        #load Checkpoint\n",
    "        check_point = torch.load(SaveTrain)\n",
    "        #Checkpoint is saved as a python dictionary\n",
    "        #https://www.w3schools.com/python/python_dictionaries.asp\n",
    "        #here we unpack the dictionary to get our previous training states\n",
    "        net.load_state_dict(check_point['model_state_dict'])\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "        optimizer.load_state_dict(check_point['optimizer_state_dict'])\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    state[k] = v.cuda()\n",
    "        start_epoch = check_point['epoch']\n",
    "        train_loss_log = check_point['train_loss']\n",
    "        train_acc_log = check_point['train_acc']\n",
    "        print(\"Training loaded, starting from epoch:\", start_epoch)\n",
    "    else:\n",
    "        #Raise Error if it does not exist\n",
    "        raise ValueError(\"Training network does not exist\")\n",
    "    if os.path.isfile(SaveVal):\n",
    "        #load Checkpoint\n",
    "        check_point = torch.load(SaveVal)\n",
    "        #Checkpoint is saved as a python dictionary\n",
    "        #https://www.w3schools.com/python/python_dictionaries.asp\n",
    "        #here we unpack the dictionary to get our previous training states\n",
    "        #net.load_state_dict(check_point['model_state_dict'])\n",
    "        #optimizer.load_state_dict(check_point['optimizer_state_dict'])\n",
    "        #start_epoch = check_point['epoch']\n",
    "        best_valid_acc = check_point['val_acc']\n",
    "        #print(\"Checkpoint loaded, starting from epoch:\", start_epoch)\n",
    "    else:\n",
    "        raise ValueError(\"Validation network does not exist\")\n",
    "else:\n",
    "    #If checkpoint does exist and Start_From_Checkpoint = False\n",
    "    #Raise an error to prevent accidental overwriting\n",
    "    if (os.path.isfile(SaveTrain) or os.path.isfile(SaveVal)):\n",
    "        raise ValueError(\"Warning Checkpoint exists\")\n",
    "    else:\n",
    "        print(\"Starting from scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): deepNet(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (batchnorm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (batchnorm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (batchnorm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (batchnorm4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv5): Conv2d(128, 384, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (inceptionA_batchnorm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionA_conv11): Conv2d(384, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (inceptionA_batchnorm11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionA_conv12): Conv2d(384, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (inceptionA_batchnorm12): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionA_conv13): Conv2d(384, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (inceptionA_batchnorm13): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionA_conv31): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (inceptionA_batchnorm31): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionA_conv32): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (inceptionA_batchnorm32): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionA_conv33): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (inceptionA_batchnorm33): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionA_conv14): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (reductionA_conv6): Conv2d(384, 1154, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (inceptionB_batchnorm): BatchNorm2d(1154, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionB_conv11): Conv2d(1154, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (inceptionB_batchnorm11): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionB_conv12): Conv2d(1154, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (inceptionB_batchnorm12): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionB_conv71): Conv2d(128, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3))\n",
       "    (inceptionB_batchnorm71): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionB_conv72): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0))\n",
       "    (inceptionB_batchnorm72): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionB_conv13): Conv2d(384, 1154, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (reductionB_conv7): Conv2d(1154, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (inceptionC_batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionC_conv11): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (inceptionC_batchnorm11): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionC_conv12): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (inceptionC_batchnorm12): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionC_conv31): Conv2d(192, 224, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "    (inceptionC_batchnorm31): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionC_conv32): Conv2d(224, 256, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "    (inceptionC_batchnorm32): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (inceptionC_conv13): Conv2d(448, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (batchnorm5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1000): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 110  |  ITER 1876/4255  |  Training loss 0.828  |  Training acc 76.562\r"
     ]
    }
   ],
   "source": [
    "# TRAIN THE NETWORK\n",
    "\n",
    "for epoch in range(start_epoch, 500):\n",
    "    \n",
    "    # run training for 1 epoch\n",
    "    train_loss, train_acc = train(net, device, criterion, optimizer, train_loader, epoch, length_train)\n",
    "    \n",
    "    train_loss_log.append(train_loss)\n",
    "    train_acc_log.append(train_acc)\n",
    "    \n",
    "    #scheduler.step()\n",
    "    \n",
    "    # run evaluation every 20 epochs\n",
    "    if not (epoch+1)%20:\n",
    "        val_loss, val_acc = evaluate(net, device, criterion, val_loader, length_val)\n",
    "        \n",
    "        # save best validation model\n",
    "        if (val_acc > best_val_acc):\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch':                    epoch,\n",
    "                'model_state_dict':         net.state_dict(),\n",
    "                'optimizer_state_dict':     optimizer.state_dict(),\n",
    "                'val_acc':                val_acc\n",
    "            }, SaveVal)\n",
    "        # save training model\n",
    "        torch.save({\n",
    "            'epoch':                    epoch,\n",
    "            'model_state_dict':         net.state_dict(),\n",
    "            'optimizer_state_dict':     optimizer.state_dict(),\n",
    "            'train_acc':                train_acc_log,\n",
    "            'train_loss':               train_loss_log\n",
    "        }, SaveTrain)\n",
    "        \n",
    "    #print(f'EPOCH {epoch+1:02}  |  Training loss {train_loss:.3f}  |  Training acc {train_acc*100:.3f}  |  Validation loss {val_loss:.3f}  |  Validation acc {val_acc*100:.3f}', end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = torch.randn(32, 2, 3)\n",
    "y = torch.randn(64, 2, 3)\n",
    "\n",
    "torch.cat((x, y), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_batch['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_batch['label'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_loss, val_acc = evaluate(net, device, criterion, val_loader, len(val_loader))\n",
    "    \n",
    "validation_loss_log.append(val_loss)\n",
    "    \n",
    "validation_acc_log.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if (val_acc > best_val_acc):\n",
    "    best_val_acc = val_acc\n",
    "    print(\"Saving model\", end='\\r')\n",
    "    torch.save({\n",
    "        'epoch':                    20,\n",
    "        'model_state_dict':         net.state_dict(),\n",
    "        'optimizer_state_dict':     optimizer.state_dict(),\n",
    "        'val_acc':                val_acc\n",
    "    }, SaveVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Saving model\", end='\\r')\n",
    "torch.save({\n",
    "    'epoch':                    74,\n",
    "    'model_state_dict':         net.state_dict(),\n",
    "    'optimizer_state_dict':     optimizer.state_dict(),\n",
    "    'train_acc':                train_acc_log,\n",
    "    'train_loss':               train_loss_log\n",
    "}, SaveTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
